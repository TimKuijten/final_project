{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install selenium\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_email = os.getenv(\"email\")\n",
    "my_password = os.getenv(\"password\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_reddit_posts(url):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--incognito\")\n",
    "    #options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the feed to load\n",
    "    WebDriverWait(driver, 10)\n",
    "    \n",
    "    while True:\n",
    "        scroll_height = 10\n",
    "        document_height_before = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "        driver.execute_script(f\"window.scrollTo(0, {document_height_before + scroll_height});\")\n",
    "        time.sleep(6)\n",
    "        document_height_after = driver.execute_script(\"return document.documentElement.scrollHeight\") \n",
    "        if document_height_after == document_height_before:\n",
    "            break\n",
    "\n",
    "\n",
    "    html = driver.execute_script(\"return document.body.outerHTML;\")\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    posts = soup.find_all(\"div\", {\"class\": \"_1poyrkZ7g36PawDueRza-J\"})\n",
    "\n",
    "    rows = []\n",
    "    for post in posts:\n",
    "        link = post.find_all('a')\n",
    "        try:\n",
    "            linkurl = 'https://www.reddit.com' + re.search(r\"/r/(\\w+)/comments/(\\w+)/(\\w+)/\", str(link)).group()\n",
    "        except:\n",
    "            linkurl = 'Error loading'\n",
    "        post_title = post.find('h3', class_='_eYtD2XCVieq6emjKBH3m')\n",
    "        rows.append([linkurl, post_title.text])\n",
    "            \n",
    "        rows.append([linkurl, post_title.text])\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return pd.DataFrame(rows, columns=['Link', \"Title\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit1 = extract_reddit_posts('https://www.reddit.com/search/?q=Artificial%20Intelligence&type=link')\n",
    "df_reddit1.drop_duplicates(inplace=True)\n",
    "df_reddit1.to_csv('df_reddit1.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit2 = extract_reddit_posts('https://www.reddit.com/search/?q=Artificial%20Intelligence&type=link&sort=comments')\n",
    "df_reddit2.drop_duplicates(inplace=True)\n",
    "df_reddit2.to_csv('df_reddit2.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit3 = extract_reddit_posts('https://www.reddit.com/search/?q=Artificial%20Intelligence&type=link&sort=new')\n",
    "df_reddit3.drop_duplicates(inplace=True)\n",
    "df_reddit3.to_csv('df_reddit3.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit4 = extract_reddit_posts('https://www.reddit.com/search/?q=Artificial%20Intelligence&type=link&sort=top')\n",
    "df_reddit4.drop_duplicates(inplace=True)\n",
    "df_reddit4.to_csv('df_reddit4.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit5 = extract_reddit_posts('https://www.reddit.com/search/?q=Artificial%20Intelligence&type=link&sort=hot')\n",
    "df_reddit5.drop_duplicates(inplace=True)\n",
    "df_reddit5.to_csv('df_reddit5.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit1.to_csv('df_reddit1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Link</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.reddit.com/r/Futurology/comments/1...</td>\n",
       "      <td>ChatGPT is just the beginning: Artificial inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://www.reddit.com/r/conspiracy/comments/1...</td>\n",
       "      <td>We need to discuss Artificial Intelligence. As...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>https://www.reddit.com/r/IAmA/comments/10pi1d4...</td>\n",
       "      <td>I'm Professor Toby Walsh, a leading artificial...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>https://www.reddit.com/r/Futurology/comments/1...</td>\n",
       "      <td>US experts warn AI likely to kill off jobs – a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>https://www.reddit.com/r/Futurology/comments/1...</td>\n",
       "      <td>Jobs With the Lowest Risk of Automation by Art...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                               Link  \\\n",
       "0      0  https://www.reddit.com/r/Futurology/comments/1...   \n",
       "1      2  https://www.reddit.com/r/conspiracy/comments/1...   \n",
       "2      4  https://www.reddit.com/r/IAmA/comments/10pi1d4...   \n",
       "3      6  https://www.reddit.com/r/Futurology/comments/1...   \n",
       "4      8  https://www.reddit.com/r/Futurology/comments/1...   \n",
       "\n",
       "                                               Title  \n",
       "0  ChatGPT is just the beginning: Artificial inte...  \n",
       "1  We need to discuss Artificial Intelligence. As...  \n",
       "2  I'm Professor Toby Walsh, a leading artificial...  \n",
       "3  US experts warn AI likely to kill off jobs – a...  \n",
       "4  Jobs With the Lowest Risk of Automation by Art...  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_red1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Content</th>\n",
       "      <th>Author</th>\n",
       "      <th>Date</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>comments</th>\n",
       "      <th>awards</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Content, Author, Date, upvotes, comments, awards]\n",
       "Index: []"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def extract_reddit_post_info(url):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--incognito\")\n",
    "    #options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    html = driver.execute_script(\"return document.body.outerHTML;\")\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    post_container = soup.find(\"div\", {\"data-testid\": \"post-container\"})\n",
    "\n",
    "    title = post_container.find(\"h1\", {\"class\": \"_eYtD2XCVieq6emjKBH3m\"})\n",
    "    username_link = post_container.find(\"a\", {\"class\": \"_2tbHP6ZydRpjI44J3syuqC\"})\n",
    "    username = username_link.text.strip(\"u/\")\n",
    "    message = post_container.find(\"div\", {\"class\": \"_292iotee39Lmt0MkQZ2hPV RichTextJSON-root\"}).text\n",
    "    date = post_container.find(\"span\", {\"class\": \"_2VF2J19pUIMSLJFky-7PEI\"}).text\n",
    "    upvotes = post_container.find(\"div\", {\"class\": \"_1E9mcoVn4MYnuBQSVDt1gC\"}).text\n",
    "    comments = post_container.find(\"span\", {\"class\": \"FHCV02u6Cp2zYL0fhQPsO\"}).text\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return pd.DataFrame([[title, username, message, date, upvotes, comments]], columns=[\"Title\", \"Username\", \"Message\", \"Date\", \"Upvotes\", \"Comments\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title      Username  \\\n",
      "0  [We need to discuss Artificial Intelligence. A...  Hodlthehodl1   \n",
      "\n",
      "                                             Message          Date Upvotes  \\\n",
      "0  To be clear, I'm not talking about the skynet ...  2 months ago     916   \n",
      "\n",
      "       Comments  \n",
      "0  466 comments  \n"
     ]
    }
   ],
   "source": [
    "df = extract_reddit_post_info(\"https://www.reddit.com/r/conspiracy/comments/109ew7w/we_need_to_discuss_artificial_intelligence_as_a/\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Username</th>\n",
       "      <th>Message</th>\n",
       "      <th>Date</th>\n",
       "      <th>Upvotes</th>\n",
       "      <th>Comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[We need to discuss Artificial Intelligence. A...</td>\n",
       "      <td>Hodlthehodl1</td>\n",
       "      <td>To be clear, I'm not talking about the skynet ...</td>\n",
       "      <td>2 months ago</td>\n",
       "      <td>916</td>\n",
       "      <td>466 comments</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title      Username  \\\n",
       "0  [We need to discuss Artificial Intelligence. A...  Hodlthehodl1   \n",
       "\n",
       "                                             Message          Date Upvotes  \\\n",
       "0  To be clear, I'm not talking about the skynet ...  2 months ago     916   \n",
       "\n",
       "       Comments  \n",
       "0  466 comments  "
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_reddit_post_info(df):\n",
    "    reddit_comments = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        url = df['Link'][i]\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\"--incognito\")\n",
    "        #options.add_argument(\"--headless\")\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.get(url)\n",
    "\n",
    "        html = driver.execute_script(\"return document.body.outerHTML;\")\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        post_container = soup.find(\"div\", {\"data-testid\": \"post-container\"})\n",
    "\n",
    "        try:\n",
    "            title = post_container.find(\"h1\", {\"class\": \"_eYtD2XCVieq6emjKBH3m\"})\n",
    "            username_link = post_container.find(\"a\", {\"class\": \"_2tbHP6ZydRpjI44J3syuqC\"})\n",
    "            username = username_link.text.strip(\"u/\")\n",
    "            message = post_container.find(\"div\", {\"class\": \"_292iotee39Lmt0MkQZ2hPV RichTextJSON-root\"}).text\n",
    "            date = post_container.find(\"span\", {\"class\": \"_2VF2J19pUIMSLJFky-7PEI\"}).text\n",
    "            upvotes = post_container.find(\"div\", {\"class\": \"_1E9mcoVn4MYnuBQSVDt1gC\"}).text\n",
    "            comments = post_container.find(\"span\", {\"class\": \"FHCV02u6Cp2zYL0fhQPsO\"}).text\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        driver.quit()\n",
    "\n",
    "        reddit_comments.append([title, username, message, date, upvotes, comments, url])\n",
    "\n",
    "        print(f'{url} done')\n",
    "\n",
    "    return pd.DataFrame(reddit_comments, columns=[\"Title\", \"Username\", \"Message\", \"Date\", \"Upvotes\", \"Comments\", \"URL\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.reddit.com/r/Futurology/comments/10qvt8l/chatgpt_is_just_the_beginning_artificial/ done\n",
      "https://www.reddit.com/r/conspiracy/comments/109ew7w/we_need_to_discuss_artificial_intelligence_as_a/ done\n",
      "https://www.reddit.com/r/IAmA/comments/10pi1d4/im_professor_toby_walsh_a_leading_artificial/ done\n",
      "https://www.reddit.com/r/Futurology/comments/10wx3tc/us_experts_warn_ai_likely_to_kill_off_jobs_and/ done\n",
      "https://www.reddit.com/r/Futurology/comments/11ck3fb/jobs_with_the_lowest_risk_of_automation_by/ done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Username</th>\n",
       "      <th>Message</th>\n",
       "      <th>Date</th>\n",
       "      <th>Upvotes</th>\n",
       "      <th>Comments</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[ChatGPT is just the beginning: Artificial int...</td>\n",
       "      <td>Gari_305</td>\n",
       "      <td></td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>14.9k</td>\n",
       "      <td>2.1k comments</td>\n",
       "      <td>https://www.reddit.com/r/Futurology/comments/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[We need to discuss Artificial Intelligence. A...</td>\n",
       "      <td>Hodlthehodl1</td>\n",
       "      <td>To be clear, I'm not talking about the skynet ...</td>\n",
       "      <td>2 months ago</td>\n",
       "      <td>908</td>\n",
       "      <td>466 comments</td>\n",
       "      <td>https://www.reddit.com/r/conspiracy/comments/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[I'm Professor Toby Walsh, a leading artificia...</td>\n",
       "      <td>nsw</td>\n",
       "      <td>Hi Reddit, Prof Toby Walsh here, keen to chat ...</td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>4.9k</td>\n",
       "      <td>1.2k comments</td>\n",
       "      <td>https://www.reddit.com/r/IAmA/comments/10pi1d4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[US experts warn AI likely to kill off jobs – ...</td>\n",
       "      <td>Gari_305</td>\n",
       "      <td></td>\n",
       "      <td>1 month ago</td>\n",
       "      <td>2.5k</td>\n",
       "      <td>557 comments</td>\n",
       "      <td>https://www.reddit.com/r/Futurology/comments/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[Jobs With the Lowest Risk of Automation by Ar...</td>\n",
       "      <td>WhiteyKC</td>\n",
       "      <td></td>\n",
       "      <td>15 days ago</td>\n",
       "      <td>423</td>\n",
       "      <td>351 comments</td>\n",
       "      <td>https://www.reddit.com/r/Futurology/comments/1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title      Username  \\\n",
       "0  [ChatGPT is just the beginning: Artificial int...      Gari_305   \n",
       "1  [We need to discuss Artificial Intelligence. A...  Hodlthehodl1   \n",
       "2  [I'm Professor Toby Walsh, a leading artificia...           nsw   \n",
       "3  [US experts warn AI likely to kill off jobs – ...      Gari_305   \n",
       "4  [Jobs With the Lowest Risk of Automation by Ar...      WhiteyKC   \n",
       "\n",
       "                                             Message          Date Upvotes  \\\n",
       "0                                                      1 month ago   14.9k   \n",
       "1  To be clear, I'm not talking about the skynet ...  2 months ago     908   \n",
       "2  Hi Reddit, Prof Toby Walsh here, keen to chat ...   1 month ago    4.9k   \n",
       "3                                                      1 month ago    2.5k   \n",
       "4                                                      15 days ago     423   \n",
       "\n",
       "        Comments                                                URL  \n",
       "0  2.1k comments  https://www.reddit.com/r/Futurology/comments/1...  \n",
       "1   466 comments  https://www.reddit.com/r/conspiracy/comments/1...  \n",
       "2  1.2k comments  https://www.reddit.com/r/IAmA/comments/10pi1d4...  \n",
       "3   557 comments  https://www.reddit.com/r/Futurology/comments/1...  \n",
       "4   351 comments  https://www.reddit.com/r/Futurology/comments/1...  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_reddit_post_info(df_red1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_reddit_post_comments(url):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--incognito\")\n",
    "    #options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    html = driver.execute_script(\"return document.body.outerHTML;\")\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    feed = soup.find('shreddit-comment-tree', {'id':\"comment-tree\"})\n",
    "    post_container = feed.find(\"delayed-dialog\", {\"class\": 'visible'})\n",
    "\n",
    "    for post in post_container:\n",
    "        text = post.find('div', {'id': '-post-rtjson-content'})\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return pd.DataFrame([[text]], columns=[\"text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_reddit_post_comments(df):\n",
    "    reddit_post_comments = []\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        url = df['Link'][i]\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\"--incognito\")\n",
    "        #options.add_argument(\"--headless\")\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.get(url)\n",
    "\n",
    "        time.sleep(5)\n",
    "\n",
    "        while True:\n",
    "            scroll_height = 10\n",
    "            document_height_before = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "            driver.execute_script(f\"window.scrollTo(0, {document_height_before + scroll_height});\")\n",
    "            time.sleep(3)\n",
    "            document_height_after = driver.execute_script(\"return document.documentElement.scrollHeight\") \n",
    "            if document_height_after == document_height_before:\n",
    "                break\n",
    "\n",
    "        html = driver.execute_script(\"return document.body.outerHTML;\")\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        driver.quit()\n",
    "        posts = soup.find_all('div', {'class': '_3tw__eCCe7j-epNCKGXUKk'})\n",
    "\n",
    "        for post in posts:\n",
    "            try:\n",
    "                name = post.find('a', {'class': 'wM6scouPXXsFDSZmZPHRo DjcdNGtVXPcxG0yiFXIoZ _23wugcdiaj44hdfugIAlnX'}).text\n",
    "            except:\n",
    "                name = post.find('a', {'class': 'wM6scouPXXsFDSZmZPHRo DjcdNGtVXPcxG0yiFXIoZ _23wugcdiaj44hdfugIAlnX'})\n",
    "\n",
    "            try:\n",
    "                date = post.find('a', class_='_3yx4Dn0W3Yunucf5sVJeFU').text\n",
    "            except:\n",
    "                date = post.find('a', class_='_3yx4Dn0W3Yunucf5sVJeFU')\n",
    "            \n",
    "            try:\n",
    "                upvotes = post.find('div', {'class':\"_1rZYMD_4xY3gRcSS3p8ODO _25IkBM0rRUqWX5ZojEMAFQ _3ChHiOyYyUkpZ_Nm3ZyM2M\"}).text\n",
    "            except:\n",
    "                upvotes = post.find('div', {'class':\"_1rZYMD_4xY3gRcSS3p8ODO _25IkBM0rRUqWX5ZojEMAFQ _3ChHiOyYyUkpZ_Nm3ZyM2M\"})\n",
    "\n",
    "            try:\n",
    "                comment_thread = post.find('p', class_='_1qeIAgB0cPwnLhDF9XSiJM').text\n",
    "            except:\n",
    "                comment_thread = post.find('p', class_='_1qeIAgB0cPwnLhDF9XSiJM')\n",
    "            reddit_post_comments.append([name, date, upvotes, comment_thread, url])\n",
    "\n",
    "    df_reddit_post_comments2 = pd.DataFrame(reddit_post_comments, columns=[\"Name\", 'date', 'upvotes', 'comment2', 'url'])\n",
    "    return df_reddit_post_comments2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>date</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>comment2</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FuturologyBot</td>\n",
       "      <td>1 mo. ago</td>\n",
       "      <td>Vote</td>\n",
       "      <td>The following submission statement was provide...</td>\n",
       "      <td>https://www.reddit.com/r/Futurology/comments/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CaptPants</td>\n",
       "      <td>1 mo. ago</td>\n",
       "      <td>4.7k</td>\n",
       "      <td>I hope it's used for more than just cutting jo...</td>\n",
       "      <td>https://www.reddit.com/r/Futurology/comments/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shanhaevel</td>\n",
       "      <td>1 mo. ago</td>\n",
       "      <td>2.0k</td>\n",
       "      <td>Haha, that's rich. As if.</td>\n",
       "      <td>https://www.reddit.com/r/Futurology/comments/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SatiricalComment</td>\n",
       "      <td>1 mo. ago</td>\n",
       "      <td>1.1k</td>\n",
       "      <td>One of the intents of many scientists who deve...</td>\n",
       "      <td>https://www.reddit.com/r/Futurology/comments/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fistfulloframen</td>\n",
       "      <td>1 mo. ago</td>\n",
       "      <td>30</td>\n",
       "      <td>You can use it to fix up your resume after you...</td>\n",
       "      <td>https://www.reddit.com/r/Futurology/comments/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>HairyChest69</td>\n",
       "      <td>14 days ago</td>\n",
       "      <td>1</td>\n",
       "      <td>I always wonder about retail workers. The peop...</td>\n",
       "      <td>https://www.reddit.com/r/Futurology/comments/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>AnEggscellentName</td>\n",
       "      <td>14 days ago</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.reddit.com/r/Futurology/comments/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>TheWaterDoctor</td>\n",
       "      <td>14 days ago</td>\n",
       "      <td>1</td>\n",
       "      <td>Building services and maintenance.</td>\n",
       "      <td>https://www.reddit.com/r/Futurology/comments/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>Unrigg3D</td>\n",
       "      <td>14 days ago</td>\n",
       "      <td>1</td>\n",
       "      <td>Nobody is at risk for not having jobs. This is...</td>\n",
       "      <td>https://www.reddit.com/r/Futurology/comments/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>cellrdoor2</td>\n",
       "      <td>14 days ago</td>\n",
       "      <td>1</td>\n",
       "      <td>As a designer in the entertainment industry, t...</td>\n",
       "      <td>https://www.reddit.com/r/Futurology/comments/1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>710 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Name         date upvotes  \\\n",
       "0        FuturologyBot    1 mo. ago    Vote   \n",
       "1            CaptPants    1 mo. ago    4.7k   \n",
       "2           Shanhaevel    1 mo. ago    2.0k   \n",
       "3     SatiricalComment    1 mo. ago    1.1k   \n",
       "4      fistfulloframen    1 mo. ago      30   \n",
       "..                 ...          ...     ...   \n",
       "705       HairyChest69  14 days ago       1   \n",
       "706  AnEggscellentName  14 days ago    None   \n",
       "707     TheWaterDoctor  14 days ago       1   \n",
       "708           Unrigg3D  14 days ago       1   \n",
       "709         cellrdoor2  14 days ago       1   \n",
       "\n",
       "                                              comment2  \\\n",
       "0    The following submission statement was provide...   \n",
       "1    I hope it's used for more than just cutting jo...   \n",
       "2                            Haha, that's rich. As if.   \n",
       "3    One of the intents of many scientists who deve...   \n",
       "4    You can use it to fix up your resume after you...   \n",
       "..                                                 ...   \n",
       "705  I always wonder about retail workers. The peop...   \n",
       "706                                               None   \n",
       "707                 Building services and maintenance.   \n",
       "708  Nobody is at risk for not having jobs. This is...   \n",
       "709  As a designer in the entertainment industry, t...   \n",
       "\n",
       "                                                   url  \n",
       "0    https://www.reddit.com/r/Futurology/comments/1...  \n",
       "1    https://www.reddit.com/r/Futurology/comments/1...  \n",
       "2    https://www.reddit.com/r/Futurology/comments/1...  \n",
       "3    https://www.reddit.com/r/Futurology/comments/1...  \n",
       "4    https://www.reddit.com/r/Futurology/comments/1...  \n",
       "..                                                 ...  \n",
       "705  https://www.reddit.com/r/Futurology/comments/1...  \n",
       "706  https://www.reddit.com/r/Futurology/comments/1...  \n",
       "707  https://www.reddit.com/r/Futurology/comments/1...  \n",
       "708  https://www.reddit.com/r/Futurology/comments/1...  \n",
       "709  https://www.reddit.com/r/Futurology/comments/1...  \n",
       "\n",
       "[710 rows x 5 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_reddit_post_comments(df_red1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
