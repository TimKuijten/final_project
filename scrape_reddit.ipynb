{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install selenium\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_email = os.getenv(\"email\")\n",
    "my_password = os.getenv(\"password\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_reddit_posts(url):\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument(\"--incognito\")\n",
    "    #options.add_argument(\"--headless\")\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "    driver = webdriver.Chrome(options=options)\n",
    "\n",
    "    driver.get(url)\n",
    "\n",
    "    # Wait for the feed to load\n",
    "    WebDriverWait(driver, 10)\n",
    "    \n",
    "    while True:\n",
    "        scroll_height = 10\n",
    "        document_height_before = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "        driver.execute_script(f\"window.scrollTo(0, {document_height_before + scroll_height});\")\n",
    "        time.sleep(6)\n",
    "        document_height_after = driver.execute_script(\"return document.documentElement.scrollHeight\") \n",
    "        if document_height_after == document_height_before:\n",
    "            break\n",
    "\n",
    "\n",
    "    html = driver.execute_script(\"return document.body.outerHTML;\")\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    posts = soup.find_all(\"div\", {\"class\": \"_1poyrkZ7g36PawDueRza-J\"})\n",
    "\n",
    "    rows = []\n",
    "    for post in posts:\n",
    "        link = post.find_all('a')\n",
    "        try:\n",
    "            linkurl = 'https://www.reddit.com' + re.search(r\"/r/(\\w+)/comments/(\\w+)/(\\w+)/\", str(link)).group()\n",
    "        except:\n",
    "            linkurl = 'Error loading'\n",
    "        post_title = post.find('h3', class_='_eYtD2XCVieq6emjKBH3m')\n",
    "        rows.append([linkurl, post_title.text])\n",
    "            \n",
    "        rows.append([linkurl, post_title.text])\n",
    "\n",
    "    driver.quit()\n",
    "\n",
    "    return pd.DataFrame(rows, columns=['Link', \"Title\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_reddit_post_info(df):\n",
    "    reddit_comments = []\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        url = row['Link']\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\"--incognito\")\n",
    "        #options.add_argument(\"--headless\")\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.get(url)\n",
    "\n",
    "        html = driver.execute_script(\"return document.body.outerHTML;\")\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "        post_container = soup.find(\"div\", {\"data-testid\": \"post-container\"})\n",
    "\n",
    "        try:\n",
    "            title = post_container.find(\"h1\", {\"class\": \"_eYtD2XCVieq6emjKBH3m\"})\n",
    "            username_link = post_container.find(\"a\", {\"class\": \"_2tbHP6ZydRpjI44J3syuqC\"})\n",
    "            username = username_link.text.strip(\"u/\")\n",
    "            message = post_container.find(\"div\", {\"class\": \"_292iotee39Lmt0MkQZ2hPV RichTextJSON-root\"}).text\n",
    "            date = post_container.find(\"span\", {\"class\": \"_2VF2J19pUIMSLJFky-7PEI\"}).text\n",
    "            upvotes = post_container.find(\"div\", {\"class\": \"_1E9mcoVn4MYnuBQSVDt1gC\"}).text\n",
    "            comments = post_container.find(\"span\", {\"class\": \"FHCV02u6Cp2zYL0fhQPsO\"}).text\n",
    "            reddit_comments.append([title, username, message, date, upvotes, comments, url])\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        driver.quit()\n",
    "\n",
    "        print(f'{url} done')\n",
    "\n",
    "    return pd.DataFrame(reddit_comments, columns=[\"Title\", \"Username\", \"Message\", \"Date\", \"Upvotes\", \"Comments\", \"URL\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_reddit_post_comments(df):\n",
    "    reddit_post_comments = []\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        url = row['Link']\n",
    "        options = webdriver.ChromeOptions()\n",
    "        options.add_argument(\"--incognito\")\n",
    "        #options.add_argument(\"--headless\")\n",
    "        options.add_argument(\"--disable-gpu\")\n",
    "\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.get(url)\n",
    "\n",
    "        time.sleep(5)\n",
    "\n",
    "        while True:\n",
    "            scroll_height = 10\n",
    "            document_height_before = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "            driver.execute_script(f\"window.scrollTo(0, {document_height_before + scroll_height});\")\n",
    "            time.sleep(3)\n",
    "            document_height_after = driver.execute_script(\"return document.documentElement.scrollHeight\") \n",
    "            if document_height_after == document_height_before:\n",
    "                break\n",
    "\n",
    "        html = driver.execute_script(\"return document.body.outerHTML;\")\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        driver.quit()\n",
    "        posts = soup.find_all('div', {'class': '_3tw__eCCe7j-epNCKGXUKk'})\n",
    "\n",
    "        for post in posts:\n",
    "            try:\n",
    "                name = post.find('a', {'class': 'wM6scouPXXsFDSZmZPHRo DjcdNGtVXPcxG0yiFXIoZ _23wugcdiaj44hdfugIAlnX'}).text\n",
    "            except:\n",
    "                name = post.find('a', {'class': 'wM6scouPXXsFDSZmZPHRo DjcdNGtVXPcxG0yiFXIoZ _23wugcdiaj44hdfugIAlnX'})\n",
    "\n",
    "            try:\n",
    "                date = post.find('a', class_='_3yx4Dn0W3Yunucf5sVJeFU').text\n",
    "            except:\n",
    "                date = post.find('a', class_='_3yx4Dn0W3Yunucf5sVJeFU')\n",
    "            \n",
    "            try:\n",
    "                upvotes = post.find('div', {'class':\"_1rZYMD_4xY3gRcSS3p8ODO _25IkBM0rRUqWX5ZojEMAFQ _3ChHiOyYyUkpZ_Nm3ZyM2M\"}).text\n",
    "            except:\n",
    "                upvotes = post.find('div', {'class':\"_1rZYMD_4xY3gRcSS3p8ODO _25IkBM0rRUqWX5ZojEMAFQ _3ChHiOyYyUkpZ_Nm3ZyM2M\"})\n",
    "\n",
    "            try:\n",
    "                comment_thread = post.find('p', class_='_1qeIAgB0cPwnLhDF9XSiJM').text\n",
    "            except:\n",
    "                comment_thread = post.find('p', class_='_1qeIAgB0cPwnLhDF9XSiJM')\n",
    "            reddit_post_comments.append([name, date, upvotes, comment_thread, url])\n",
    "\n",
    "    df_reddit_post_comments2 = pd.DataFrame(reddit_post_comments, columns=[\"Name\", 'date', 'upvotes', 'comment2', 'url'])\n",
    "    return df_reddit_post_comments2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit1 = extract_reddit_posts('https://www.reddit.com/search/?q=Artificial%20Intelligence&type=link')\n",
    "df_reddit1.drop_duplicates(inplace=True)\n",
    "df_reddit1.to_csv('df_reddit1.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit2 = extract_reddit_posts('https://www.reddit.com/search/?q=Artificial%20Intelligence&type=link&sort=comments')\n",
    "df_reddit2.drop_duplicates(inplace=True)\n",
    "df_reddit2.to_csv('df_reddit2.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit3 = extract_reddit_posts('https://www.reddit.com/search/?q=Artificial%20Intelligence&type=link&sort=new')\n",
    "df_reddit3.drop_duplicates(inplace=True)\n",
    "df_reddit3.to_csv('df_reddit3.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit4 = extract_reddit_posts('https://www.reddit.com/search/?q=Artificial%20Intelligence&type=link&sort=top')\n",
    "df_reddit4.drop_duplicates(inplace=True)\n",
    "df_reddit4.to_csv('df_reddit4.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit5 = extract_reddit_posts('https://www.reddit.com/search/?q=Artificial%20Intelligence&type=link&sort=hot')\n",
    "df_reddit5.drop_duplicates(inplace=True)\n",
    "df_reddit5.to_csv('df_reddit5.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit1 = pd.read_csv('df_reddit1.csv')\n",
    "df_reddit2 = pd.read_csv('df_reddit2.csv')\n",
    "df_reddit3 = pd.read_csv('df_reddit3.csv')\n",
    "df_reddit4 = pd.read_csv('df_reddit4.csv')\n",
    "df_reddit5 = pd.read_csv('df_reddit5.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reddit1 = df_reddit1[df_reddit1['Link'] != 'Error loading']\n",
    "df_reddit2 = df_reddit2[df_reddit2['Link'] != 'Error loading']\n",
    "df_reddit3 = df_reddit3[df_reddit3['Link'] != 'Error loading']\n",
    "df_reddit4 = df_reddit4[df_reddit4['Link'] != 'Error loading']\n",
    "df_reddit5 = df_reddit5[df_reddit5['Link'] != 'Error loading']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.reddit.com/r/Futurology/comments/10qvt8l/chatgpt_is_just_the_beginning_artificial/ done\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Tim_K\\miniconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\Tim_K\\miniconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Tim_K\\miniconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:2263\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:2273\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Tim_K\\Ironhack\\final_project\\scrape_reddit.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Tim_K/Ironhack/final_project/scrape_reddit.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m erpi1 \u001b[39m=\u001b[39m extract_reddit_post_info(df_reddit1)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Tim_K/Ironhack/final_project/scrape_reddit.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m erpi1\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39merpi1.csv\u001b[39m\u001b[39m'\u001b[39m, index \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32mc:\\Users\\Tim_K\\Ironhack\\final_project\\scrape_reddit.ipynb Cell 11\u001b[0m in \u001b[0;36mextract_reddit_post_info\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Tim_K/Ironhack/final_project/scrape_reddit.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m reddit_comments \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Tim_K/Ironhack/final_project/scrape_reddit.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(df)):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Tim_K/Ironhack/final_project/scrape_reddit.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     url \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mLink\u001b[39;49m\u001b[39m'\u001b[39;49m][i]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Tim_K/Ironhack/final_project/scrape_reddit.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     options \u001b[39m=\u001b[39m webdriver\u001b[39m.\u001b[39mChromeOptions()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Tim_K/Ironhack/final_project/scrape_reddit.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     options\u001b[39m.\u001b[39madd_argument(\u001b[39m\"\u001b[39m\u001b[39m--incognito\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Tim_K\\miniconda3\\lib\\site-packages\\pandas\\core\\series.py:981\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    978\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[0;32m    980\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m--> 981\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[0;32m    983\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[0;32m    984\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[0;32m    985\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    986\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tim_K\\miniconda3\\lib\\site-packages\\pandas\\core\\series.py:1089\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1086\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[0;32m   1088\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1089\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[0;32m   1090\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_get_values_for_loc(\u001b[39mself\u001b[39m, loc, label)\n",
      "File \u001b[1;32mc:\\Users\\Tim_K\\miniconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "erpi1 = extract_reddit_post_info(df_reddit1)\n",
    "erpi1.to_csv('erpi1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.reddit.com/r/pcmasterrace/comments/wddcpe/giveaway_giving_away_10_deskmats_from_the_ai/ done\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Tim_K\\miniconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[1;32mc:\\Users\\Tim_K\\miniconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Tim_K\\miniconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:2263\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:2273\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Tim_K\\Ironhack\\final_project\\scrape_reddit.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Tim_K/Ironhack/final_project/scrape_reddit.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m erpi2 \u001b[39m=\u001b[39mextract_reddit_post_info(df_reddit2)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Tim_K/Ironhack/final_project/scrape_reddit.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m erpi2\u001b[39m.\u001b[39mto_csv(\u001b[39m'\u001b[39m\u001b[39merpi2.csv\u001b[39m\u001b[39m'\u001b[39m, index \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32mc:\\Users\\Tim_K\\Ironhack\\final_project\\scrape_reddit.ipynb Cell 12\u001b[0m in \u001b[0;36mextract_reddit_post_info\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Tim_K/Ironhack/final_project/scrape_reddit.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m reddit_comments \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Tim_K/Ironhack/final_project/scrape_reddit.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(df)):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Tim_K/Ironhack/final_project/scrape_reddit.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     url \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mLink\u001b[39;49m\u001b[39m'\u001b[39;49m][i]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Tim_K/Ironhack/final_project/scrape_reddit.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     options \u001b[39m=\u001b[39m webdriver\u001b[39m.\u001b[39mChromeOptions()\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Tim_K/Ironhack/final_project/scrape_reddit.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     options\u001b[39m.\u001b[39madd_argument(\u001b[39m\"\u001b[39m\u001b[39m--incognito\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Tim_K\\miniconda3\\lib\\site-packages\\pandas\\core\\series.py:981\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    978\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[0;32m    980\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m--> 981\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[0;32m    983\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[0;32m    984\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[0;32m    985\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    986\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Tim_K\\miniconda3\\lib\\site-packages\\pandas\\core\\series.py:1089\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1086\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[0;32m   1088\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1089\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[0;32m   1090\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_get_values_for_loc(\u001b[39mself\u001b[39m, loc, label)\n",
      "File \u001b[1;32mc:\\Users\\Tim_K\\miniconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "erpi2 =extract_reddit_post_info(df_reddit2)\n",
    "erpi2.to_csv('erpi2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erpi3 = extract_reddit_post_info(df_reddit3)\n",
    "erpi3.to_csv('erpi3.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erpi4 = extract_reddit_post_info(df_reddit4)\n",
    "erpi4.to_csv('erpi4.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erpi5 = extract_reddit_post_info(df_reddit5)\n",
    "erpi5.to_csv('erpi5.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>date</th>\n",
       "      <th>upvotes</th>\n",
       "      <th>comment2</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>FuturologyBot</td>\n",
       "      <td>1 mo. ago</td>\n",
       "      <td>Vote</td>\n",
       "      <td>The following submission statement was provide...</td>\n",
       "      <td>https://www.reddit.com/r/Futurology/comments/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CaptPants</td>\n",
       "      <td>1 mo. ago</td>\n",
       "      <td>4.7k</td>\n",
       "      <td>I hope it's used for more than just cutting jo...</td>\n",
       "      <td>https://www.reddit.com/r/Futurology/comments/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Shanhaevel</td>\n",
       "      <td>1 mo. ago</td>\n",
       "      <td>2.0k</td>\n",
       "      <td>Haha, that's rich. As if.</td>\n",
       "      <td>https://www.reddit.com/r/Futurology/comments/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SatiricalComment</td>\n",
       "      <td>1 mo. ago</td>\n",
       "      <td>1.1k</td>\n",
       "      <td>One of the intents of many scientists who deve...</td>\n",
       "      <td>https://www.reddit.com/r/Futurology/comments/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fistfulloframen</td>\n",
       "      <td>1 mo. ago</td>\n",
       "      <td>30</td>\n",
       "      <td>You can use it to fix up your resume after you...</td>\n",
       "      <td>https://www.reddit.com/r/Futurology/comments/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>705</th>\n",
       "      <td>HairyChest69</td>\n",
       "      <td>14 days ago</td>\n",
       "      <td>1</td>\n",
       "      <td>I always wonder about retail workers. The peop...</td>\n",
       "      <td>https://www.reddit.com/r/Futurology/comments/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>706</th>\n",
       "      <td>AnEggscellentName</td>\n",
       "      <td>14 days ago</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>https://www.reddit.com/r/Futurology/comments/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>707</th>\n",
       "      <td>TheWaterDoctor</td>\n",
       "      <td>14 days ago</td>\n",
       "      <td>1</td>\n",
       "      <td>Building services and maintenance.</td>\n",
       "      <td>https://www.reddit.com/r/Futurology/comments/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>708</th>\n",
       "      <td>Unrigg3D</td>\n",
       "      <td>14 days ago</td>\n",
       "      <td>1</td>\n",
       "      <td>Nobody is at risk for not having jobs. This is...</td>\n",
       "      <td>https://www.reddit.com/r/Futurology/comments/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>709</th>\n",
       "      <td>cellrdoor2</td>\n",
       "      <td>14 days ago</td>\n",
       "      <td>1</td>\n",
       "      <td>As a designer in the entertainment industry, t...</td>\n",
       "      <td>https://www.reddit.com/r/Futurology/comments/1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>710 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Name         date upvotes  \\\n",
       "0        FuturologyBot    1 mo. ago    Vote   \n",
       "1            CaptPants    1 mo. ago    4.7k   \n",
       "2           Shanhaevel    1 mo. ago    2.0k   \n",
       "3     SatiricalComment    1 mo. ago    1.1k   \n",
       "4      fistfulloframen    1 mo. ago      30   \n",
       "..                 ...          ...     ...   \n",
       "705       HairyChest69  14 days ago       1   \n",
       "706  AnEggscellentName  14 days ago    None   \n",
       "707     TheWaterDoctor  14 days ago       1   \n",
       "708           Unrigg3D  14 days ago       1   \n",
       "709         cellrdoor2  14 days ago       1   \n",
       "\n",
       "                                              comment2  \\\n",
       "0    The following submission statement was provide...   \n",
       "1    I hope it's used for more than just cutting jo...   \n",
       "2                            Haha, that's rich. As if.   \n",
       "3    One of the intents of many scientists who deve...   \n",
       "4    You can use it to fix up your resume after you...   \n",
       "..                                                 ...   \n",
       "705  I always wonder about retail workers. The peop...   \n",
       "706                                               None   \n",
       "707                 Building services and maintenance.   \n",
       "708  Nobody is at risk for not having jobs. This is...   \n",
       "709  As a designer in the entertainment industry, t...   \n",
       "\n",
       "                                                   url  \n",
       "0    https://www.reddit.com/r/Futurology/comments/1...  \n",
       "1    https://www.reddit.com/r/Futurology/comments/1...  \n",
       "2    https://www.reddit.com/r/Futurology/comments/1...  \n",
       "3    https://www.reddit.com/r/Futurology/comments/1...  \n",
       "4    https://www.reddit.com/r/Futurology/comments/1...  \n",
       "..                                                 ...  \n",
       "705  https://www.reddit.com/r/Futurology/comments/1...  \n",
       "706  https://www.reddit.com/r/Futurology/comments/1...  \n",
       "707  https://www.reddit.com/r/Futurology/comments/1...  \n",
       "708  https://www.reddit.com/r/Futurology/comments/1...  \n",
       "709  https://www.reddit.com/r/Futurology/comments/1...  \n",
       "\n",
       "[710 rows x 5 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "erpc1 = extract_reddit_post_comments(df_reddit1)\n",
    "erpc1.to_csv('erpc1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "erpc2 = extract_reddit_post_comments(df_reddit2)\n",
    "erpc2.to_csv('erpc2.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Link</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.reddit.com/r/cryptostreetbets/comm...</td>\n",
       "      <td>$CUMINU the crypto version of OnlyFans. OnlyFa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.reddit.com/r/aibiz/comments/11qgze...</td>\n",
       "      <td>Best AI Newsletters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.reddit.com/r/businessanalysis/comm...</td>\n",
       "      <td>Need better ideas to sell edtech app to 1070 s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>https://www.reddit.com/r/DharmikEdits/comments...</td>\n",
       "      <td>Need better ideas to sell edtech app to 1070 s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>https://www.reddit.com/r/CryptoMoonShots/comme...</td>\n",
       "      <td>$CUMINU the crypto version of OnlyFans. OnlyFa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>https://www.reddit.com/r/sweatystartup/comment...</td>\n",
       "      <td>Need better ideas to sell edtech app to 1070 s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>https://www.reddit.com/r/IndiaSpeaks/comments/...</td>\n",
       "      <td>Need better ideas to sell edtech app to 1070 s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>https://www.reddit.com/r/EntrepreneurRideAlong...</td>\n",
       "      <td>Need better ideas to sell edtech app to 1070 s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>https://www.reddit.com/r/startup_resources/com...</td>\n",
       "      <td>Need better ideas to sell edtech app to 1070 s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>https://www.reddit.com/r/SideProject/comments/...</td>\n",
       "      <td>Need better ideas to sell edtech app to 1070 s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>https://www.reddit.com/r/Startup_Ideas/comment...</td>\n",
       "      <td>Need better ideas to sell edtech startup app t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>https://www.reddit.com/r/CryptoMarsShots/comme...</td>\n",
       "      <td>$CUMINU the crypto version of OnlyFans. OnlyFa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>https://www.reddit.com/r/CryptoMoon/comments/1...</td>\n",
       "      <td>$CUMINU the crypto version of OnlyFans. OnlyFa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>https://www.reddit.com/r/NoStupidQuestions/com...</td>\n",
       "      <td>What is the difference between Biological Inte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>https://www.reddit.com/r/Entrepreneur/comments...</td>\n",
       "      <td>Need better ideas to sell edtech app to 1070 s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Link  \\\n",
       "0   https://www.reddit.com/r/cryptostreetbets/comm...   \n",
       "2   https://www.reddit.com/r/aibiz/comments/11qgze...   \n",
       "4   https://www.reddit.com/r/businessanalysis/comm...   \n",
       "6   https://www.reddit.com/r/DharmikEdits/comments...   \n",
       "8   https://www.reddit.com/r/CryptoMoonShots/comme...   \n",
       "10  https://www.reddit.com/r/sweatystartup/comment...   \n",
       "12  https://www.reddit.com/r/IndiaSpeaks/comments/...   \n",
       "14  https://www.reddit.com/r/EntrepreneurRideAlong...   \n",
       "16  https://www.reddit.com/r/startup_resources/com...   \n",
       "18  https://www.reddit.com/r/SideProject/comments/...   \n",
       "20  https://www.reddit.com/r/Startup_Ideas/comment...   \n",
       "22  https://www.reddit.com/r/CryptoMarsShots/comme...   \n",
       "24  https://www.reddit.com/r/CryptoMoon/comments/1...   \n",
       "26  https://www.reddit.com/r/NoStupidQuestions/com...   \n",
       "28  https://www.reddit.com/r/Entrepreneur/comments...   \n",
       "\n",
       "                                                Title  \n",
       "0   $CUMINU the crypto version of OnlyFans. OnlyFa...  \n",
       "2                                 Best AI Newsletters  \n",
       "4   Need better ideas to sell edtech app to 1070 s...  \n",
       "6   Need better ideas to sell edtech app to 1070 s...  \n",
       "8   $CUMINU the crypto version of OnlyFans. OnlyFa...  \n",
       "10  Need better ideas to sell edtech app to 1070 s...  \n",
       "12  Need better ideas to sell edtech app to 1070 s...  \n",
       "14  Need better ideas to sell edtech app to 1070 s...  \n",
       "16  Need better ideas to sell edtech app to 1070 s...  \n",
       "18  Need better ideas to sell edtech app to 1070 s...  \n",
       "20  Need better ideas to sell edtech startup app t...  \n",
       "22  $CUMINU the crypto version of OnlyFans. OnlyFa...  \n",
       "24  $CUMINU the crypto version of OnlyFans. OnlyFa...  \n",
       "26  What is the difference between Biological Inte...  \n",
       "28  Need better ideas to sell edtech app to 1070 s...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reddit3.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "erpc3 = extract_reddit_post_comments(df_reddit3)\n",
    "erpc3.to_csv('erpc3.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erpc4 = extract_reddit_post_comments(df_reddit4)\n",
    "erpc4.to_csv('erpc4.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erpc5 = extract_reddit_post_comments(df_reddit5)\n",
    "erpc5.to_csv('erpc5.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erpc6 = extract_reddit_post_comments(df_reddit1)\n",
    "erpc6.to_csv('erpc5.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
